{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10861766,"sourceType":"datasetVersion","datasetId":6705158}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:03:34.862231Z","iopub.execute_input":"2025-03-11T06:03:34.862477Z","iopub.status.idle":"2025-03-11T06:03:35.073703Z","shell.execute_reply.started":"2025-03-11T06:03:34.862447Z","shell.execute_reply":"2025-03-11T06:03:35.072734Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Create dataset for Sleep and fitness","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom huggingface_hub import create_repo, upload_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:03:37.379488Z","iopub.execute_input":"2025-03-11T06:03:37.379940Z","iopub.status.idle":"2025-03-11T06:03:39.150693Z","shell.execute_reply.started":"2025-03-11T06:03:37.379901Z","shell.execute_reply":"2025-03-11T06:03:39.149621Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Define my repository\nrepo_id = \"johnjehiel/ph-llm-dataset\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:03:39.152128Z","iopub.execute_input":"2025-03-11T06:03:39.152665Z","iopub.status.idle":"2025-03-11T06:03:39.156921Z","shell.execute_reply.started":"2025-03-11T06:03:39.152629Z","shell.execute_reply":"2025-03-11T06:03:39.155576Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load the CSV dataset (assumed to have columns: 'Category', 'ID', 'Prompt', 'Response')\ncsv_path = \"/kaggle/input/sleep-and-fitness-dataset/PH-LLM Custom Dataset.csv\"\ndf = pd.read_csv(csv_path)\n\n# Stratified split by 'Category': 90% train, 10% test\ntrain_df, test_df = train_test_split(\n    df, test_size=0.1, stratify=df[\"Category\"], random_state=42\n)\n\n# Save the train and test splits as separate Parquet files\ntrain_parquet_path = \"ph-llm-dataset_train.parquet\"\ntest_parquet_path = \"ph-llm-dataset_test.parquet\"\ntrain_df.to_parquet(train_parquet_path, index=False)\ntest_df.to_parquet(test_parquet_path, index=False)\n\n# Create the dataset repository on Hugging Face (specify repo_type=\"dataset\")\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the train split to the dataset repository\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",  # IMPORTANT: ensure you're uploading to a dataset repo\n    commit_message=\"Upload train split (90%) for sleep and fitness\"\n)\n\n# Upload the test split to the dataset repository\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",  # IMPORTANT: ensure you're uploading to a dataset repo\n    commit_message=\"Upload test split (10%) for sleep and fitness\"\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T06:57:05.389238Z","iopub.execute_input":"2025-03-10T06:57:05.389658Z","iopub.status.idle":"2025-03-10T06:57:08.160021Z","shell.execute_reply.started":"2025-03-10T06:57:05.389626Z","shell.execute_reply":"2025-03-10T06:57:08.158714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nds = load_dataset(\"johnjehiel/ph-llm-dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T08:11:43.341491Z","iopub.execute_input":"2025-03-10T08:11:43.341864Z","iopub.status.idle":"2025-03-10T08:11:49.231220Z","shell.execute_reply.started":"2025-03-10T08:11:43.341833Z","shell.execute_reply":"2025-03-10T08:11:49.230285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T08:11:49.232301Z","iopub.execute_input":"2025-03-10T08:11:49.232839Z","iopub.status.idle":"2025-03-10T08:11:49.238759Z","shell.execute_reply.started":"2025-03-10T08:11:49.232808Z","shell.execute_reply":"2025-03-10T08:11:49.237731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for MMLU clinical knowledge","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, concatenate_datasets\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:04:12.813506Z","iopub.execute_input":"2025-03-11T06:04:12.813872Z","iopub.status.idle":"2025-03-11T06:04:14.015310Z","shell.execute_reply.started":"2025-03-11T06:04:12.813840Z","shell.execute_reply":"2025-03-11T06:04:14.014311Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load the mmlu clinical knowledge dataset\nmmlu_ds = load_dataset(\"openlifescienceai/mmlu_clinical_knowledge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:11:28.319552Z","iopub.execute_input":"2025-03-10T12:11:28.319834Z","iopub.status.idle":"2025-03-10T12:11:31.359423Z","shell.execute_reply.started":"2025-03-10T12:11:28.319815Z","shell.execute_reply":"2025-03-10T12:11:31.358462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_template = \"\"\"You are a medical expert {specialization}. Answer the following question by selecting the correct option.\n\n### Question:\n{question}\n\n### Options:\n(A) {option_A}\n(B) {option_B}\n(C) {option_C}\n(D) {option_D}\n\"\"\"\nresponse_template = \"Answer: ({correct_option}) {correct_answer}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:32:39.701612Z","iopub.execute_input":"2025-03-10T12:32:39.701896Z","iopub.status.idle":"2025-03-10T12:32:39.706330Z","shell.execute_reply.started":"2025-03-10T12:32:39.701877Z","shell.execute_reply":"2025-03-10T12:32:39.705221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_prompt_response(record):\n    # Extract data from the 'data' dictionary.\n    data = record[\"data\"]\n    question = data.get(\"Question\", \"\")\n    options = data.get(\"Options\", {})\n    option_A = options.get(\"A\", \"\")\n    option_B = options.get(\"B\", \"\")\n    option_C = options.get(\"C\", \"\")\n    option_D = options.get(\"D\", \"\")\n    specialization = \"specialized in \" + \" \".join(record[\"subject_name\"].split(\"_\")) if record[\"subject_name\"] else \"\"\n    prompt = prompt_template.format(\n        specialization=specialization,\n        question=question,\n        option_A=option_A,\n        option_B=option_B,\n        option_C=option_C,\n        option_D=option_D\n    )\n    correct_answer = data.get(\"Correct Answer\", \"\")\n    correct_option = data.get(\"Correct Option\", \"\")\n    response = response_template.format(\n        correct_option=correct_option,\n        correct_answer=correct_answer\n    )\n    return {\n        \"Category\": record[\"subject_name\"],\n        \"ID\": record[\"id\"],\n        \"Prompt\": prompt,\n        \"Response\": response\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:54:26.771553Z","iopub.execute_input":"2025-03-10T12:54:26.771871Z","iopub.status.idle":"2025-03-10T12:54:26.778377Z","shell.execute_reply.started":"2025-03-10T12:54:26.771843Z","shell.execute_reply":"2025-03-10T12:54:26.777009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"test\" split to promptâ€“response pairs\nconverted_test = mmlu_ds[\"test\"].map(format_prompt_response)\nconverted_test = converted_test.remove_columns(['subject_name', 'data', 'id'])\n# Convert the \"validation\" split\nconverted_validation = mmlu_ds[\"validation\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['subject_name', 'data', 'id'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_test,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:32:51.144416Z","iopub.execute_input":"2025-03-10T12:32:51.144745Z","iopub.status.idle":"2025-03-10T12:32:51.185227Z","shell.execute_reply.started":"2025-03-10T12:32:51.144725Z","shell.execute_reply":"2025-03-10T12:32:51.184093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:36:32.905558Z","iopub.execute_input":"2025-03-10T12:36:32.905857Z","iopub.status.idle":"2025-03-10T12:36:32.964473Z","shell.execute_reply.started":"2025-03-10T12:36:32.905839Z","shell.execute_reply":"2025-03-10T12:36:32.963391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:41:04.850765Z","iopub.execute_input":"2025-03-10T12:41:04.851096Z","iopub.status.idle":"2025-03-10T12:41:09.409598Z","shell.execute_reply.started":"2025-03-10T12:41:04.851067Z","shell.execute_reply":"2025-03-10T12:41:09.408270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with mmlu clinical knowledge data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with mmlu clinical knowledge data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:47:38.444661Z","iopub.execute_input":"2025-03-10T12:47:38.444991Z","iopub.status.idle":"2025-03-10T12:47:43.993872Z","shell.execute_reply.started":"2025-03-10T12:47:38.444964Z","shell.execute_reply":"2025-03-10T12:47:43.992427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:48:07.832188Z","iopub.execute_input":"2025-03-10T12:48:07.832555Z","iopub.status.idle":"2025-03-10T12:48:13.074293Z","shell.execute_reply.started":"2025-03-10T12:48:07.832527Z","shell.execute_reply":"2025-03-10T12:48:13.072906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:48:19.397770Z","iopub.execute_input":"2025-03-10T12:48:19.398092Z","iopub.status.idle":"2025-03-10T12:48:19.403191Z","shell.execute_reply.started":"2025-03-10T12:48:19.398065Z","shell.execute_reply":"2025-03-10T12:48:19.402443Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for MMLU college medicine","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"openlifescienceai/mmlu_college_medicine\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:54:32.495869Z","iopub.execute_input":"2025-03-10T12:54:32.496177Z","iopub.status.idle":"2025-03-10T12:54:39.089025Z","shell.execute_reply.started":"2025-03-10T12:54:32.496152Z","shell.execute_reply":"2025-03-10T12:54:39.087734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"test\" split to promptâ€“response pairs\nconverted_test = ds[\"test\"].map(format_prompt_response)\nconverted_test = converted_test.remove_columns(['subject_name', 'data', 'id'])\n# Convert the \"validation\" split\nconverted_validation = ds[\"validation\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['subject_name', 'data', 'id'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_test,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:55:14.933352Z","iopub.execute_input":"2025-03-10T12:55:14.933667Z","iopub.status.idle":"2025-03-10T12:55:15.014772Z","shell.execute_reply.started":"2025-03-10T12:55:14.933648Z","shell.execute_reply":"2025-03-10T12:55:15.013569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:55:45.071765Z","iopub.execute_input":"2025-03-10T12:55:45.072111Z","iopub.status.idle":"2025-03-10T12:55:45.130222Z","shell.execute_reply.started":"2025-03-10T12:55:45.072081Z","shell.execute_reply":"2025-03-10T12:55:45.128766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:57:53.289464Z","iopub.execute_input":"2025-03-10T12:57:53.289768Z","iopub.status.idle":"2025-03-10T12:57:56.201758Z","shell.execute_reply.started":"2025-03-10T12:57:53.289741Z","shell.execute_reply":"2025-03-10T12:57:56.200907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with mmlu college medicine data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with mmlu college medicine data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:00:01.119959Z","iopub.execute_input":"2025-03-10T13:00:01.120338Z","iopub.status.idle":"2025-03-10T13:00:06.499627Z","shell.execute_reply.started":"2025-03-10T13:00:01.120276Z","shell.execute_reply":"2025-03-10T13:00:06.498587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:00:22.407860Z","iopub.execute_input":"2025-03-10T13:00:22.408172Z","iopub.status.idle":"2025-03-10T13:00:27.786893Z","shell.execute_reply.started":"2025-03-10T13:00:22.408145Z","shell.execute_reply":"2025-03-10T13:00:27.785832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:01:13.770267Z","iopub.execute_input":"2025-03-10T13:01:13.770704Z","iopub.status.idle":"2025-03-10T13:01:13.776817Z","shell.execute_reply.started":"2025-03-10T13:01:13.770673Z","shell.execute_reply":"2025-03-10T13:01:13.775932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for college biology","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"openlifescienceai/mmlu_college_biology\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:03:06.025350Z","iopub.execute_input":"2025-03-10T13:03:06.025793Z","iopub.status.idle":"2025-03-10T13:03:13.130725Z","shell.execute_reply.started":"2025-03-10T13:03:06.025759Z","shell.execute_reply":"2025-03-10T13:03:13.128283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"test\" split to promptâ€“response pairs\nconverted_test = ds[\"test\"].map(format_prompt_response)\nconverted_test = converted_test.remove_columns(['subject_name', 'data', 'id'])\n# Convert the \"validation\" split\nconverted_validation = ds[\"validation\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['subject_name', 'data', 'id'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_test,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:03:29.990138Z","iopub.execute_input":"2025-03-10T13:03:29.990525Z","iopub.status.idle":"2025-03-10T13:03:30.080052Z","shell.execute_reply.started":"2025-03-10T13:03:29.990496Z","shell.execute_reply":"2025-03-10T13:03:30.079173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:03:47.435950Z","iopub.execute_input":"2025-03-10T13:03:47.436327Z","iopub.status.idle":"2025-03-10T13:03:47.490227Z","shell.execute_reply.started":"2025-03-10T13:03:47.436276Z","shell.execute_reply":"2025-03-10T13:03:47.488587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:04:12.069271Z","iopub.execute_input":"2025-03-10T13:04:12.069648Z","iopub.status.idle":"2025-03-10T13:04:14.808124Z","shell.execute_reply.started":"2025-03-10T13:04:12.069617Z","shell.execute_reply":"2025-03-10T13:04:14.806958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with mmlu college biology data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with mmlu college biology data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:06:21.559903Z","iopub.execute_input":"2025-03-10T13:06:21.560196Z","iopub.status.idle":"2025-03-10T13:06:28.409516Z","shell.execute_reply.started":"2025-03-10T13:06:21.560171Z","shell.execute_reply":"2025-03-10T13:06:28.408517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:07:46.908050Z","iopub.execute_input":"2025-03-10T13:07:46.908454Z","iopub.status.idle":"2025-03-10T13:07:52.623748Z","shell.execute_reply.started":"2025-03-10T13:07:46.908412Z","shell.execute_reply":"2025-03-10T13:07:52.622795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:07:52.624675Z","iopub.execute_input":"2025-03-10T13:07:52.624887Z","iopub.status.idle":"2025-03-10T13:07:52.629179Z","shell.execute_reply.started":"2025-03-10T13:07:52.624870Z","shell.execute_reply":"2025-03-10T13:07:52.628487Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for MMLU anatomy","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"openlifescienceai/mmlu_anatomy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:08:51.000894Z","iopub.execute_input":"2025-03-10T13:08:51.001192Z","iopub.status.idle":"2025-03-10T13:08:57.712336Z","shell.execute_reply.started":"2025-03-10T13:08:51.001173Z","shell.execute_reply":"2025-03-10T13:08:57.711586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"test\" split to promptâ€“response pairs\nconverted_test = ds[\"test\"].map(format_prompt_response)\nconverted_test = converted_test.remove_columns(['subject_name', 'data', 'id'])\n# Convert the \"validation\" split\nconverted_validation = ds[\"validation\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['subject_name', 'data', 'id'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_test,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:10:33.569329Z","iopub.execute_input":"2025-03-10T13:10:33.569663Z","iopub.status.idle":"2025-03-10T13:10:33.632994Z","shell.execute_reply.started":"2025-03-10T13:10:33.569643Z","shell.execute_reply":"2025-03-10T13:10:33.631711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:10:42.785790Z","iopub.execute_input":"2025-03-10T13:10:42.786082Z","iopub.status.idle":"2025-03-10T13:10:42.838903Z","shell.execute_reply.started":"2025-03-10T13:10:42.786062Z","shell.execute_reply":"2025-03-10T13:10:42.837951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:11:13.472878Z","iopub.execute_input":"2025-03-10T13:11:13.473225Z","iopub.status.idle":"2025-03-10T13:11:15.731136Z","shell.execute_reply.started":"2025-03-10T13:11:13.473196Z","shell.execute_reply":"2025-03-10T13:11:15.729584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with mmlu anatomy data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with mmlu anatomy data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:12:29.203248Z","iopub.execute_input":"2025-03-10T13:12:29.203637Z","iopub.status.idle":"2025-03-10T13:12:34.925952Z","shell.execute_reply.started":"2025-03-10T13:12:29.203609Z","shell.execute_reply":"2025-03-10T13:12:34.925093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:12:41.578665Z","iopub.execute_input":"2025-03-10T13:12:41.578995Z","iopub.status.idle":"2025-03-10T13:12:46.555004Z","shell.execute_reply.started":"2025-03-10T13:12:41.578967Z","shell.execute_reply":"2025-03-10T13:12:46.553762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:12:46.556187Z","iopub.execute_input":"2025-03-10T13:12:46.556602Z","iopub.status.idle":"2025-03-10T13:12:46.563043Z","shell.execute_reply.started":"2025-03-10T13:12:46.556570Z","shell.execute_reply":"2025-03-10T13:12:46.561887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for MMLU professional medicine","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"openlifescienceai/mmlu_professional_medicine\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:13:58.277455Z","iopub.execute_input":"2025-03-10T13:13:58.277764Z","iopub.status.idle":"2025-03-10T13:14:04.379193Z","shell.execute_reply.started":"2025-03-10T13:13:58.277738Z","shell.execute_reply":"2025-03-10T13:14:04.377866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"test\" split to promptâ€“response pairs\nconverted_test = ds[\"test\"].map(format_prompt_response)\nconverted_test = converted_test.remove_columns(['subject_name', 'data', 'id'])\n# Convert the \"validation\" split\nconverted_validation = ds[\"validation\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['subject_name', 'data', 'id'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_test,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:14:08.167009Z","iopub.execute_input":"2025-03-10T13:14:08.167385Z","iopub.status.idle":"2025-03-10T13:14:08.257190Z","shell.execute_reply.started":"2025-03-10T13:14:08.167350Z","shell.execute_reply":"2025-03-10T13:14:08.256340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:14:21.438917Z","iopub.execute_input":"2025-03-10T13:14:21.439214Z","iopub.status.idle":"2025-03-10T13:14:21.493644Z","shell.execute_reply.started":"2025-03-10T13:14:21.439194Z","shell.execute_reply":"2025-03-10T13:14:21.492083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:14:28.385207Z","iopub.execute_input":"2025-03-10T13:14:28.385550Z","iopub.status.idle":"2025-03-10T13:14:30.708668Z","shell.execute_reply.started":"2025-03-10T13:14:28.385529Z","shell.execute_reply":"2025-03-10T13:14:30.707278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with mmlu professional medicine data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with mmlu professional medicine data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:15:27.119341Z","iopub.execute_input":"2025-03-10T13:15:27.119708Z","iopub.status.idle":"2025-03-10T13:15:32.911612Z","shell.execute_reply.started":"2025-03-10T13:15:27.119654Z","shell.execute_reply":"2025-03-10T13:15:32.910223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:15:52.644148Z","iopub.execute_input":"2025-03-10T13:15:52.644496Z","iopub.status.idle":"2025-03-10T13:15:57.610019Z","shell.execute_reply.started":"2025-03-10T13:15:52.644468Z","shell.execute_reply":"2025-03-10T13:15:57.608690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:16:38.111862Z","iopub.execute_input":"2025-03-10T13:16:38.112217Z","iopub.status.idle":"2025-03-10T13:16:38.117797Z","shell.execute_reply.started":"2025-03-10T13:16:38.112187Z","shell.execute_reply":"2025-03-10T13:16:38.116849Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for MMLU medical genetics","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"openlifescienceai/mmlu_medical_genetics\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:17:16.919440Z","iopub.execute_input":"2025-03-10T13:17:16.919757Z","iopub.status.idle":"2025-03-10T13:17:24.610700Z","shell.execute_reply.started":"2025-03-10T13:17:16.919730Z","shell.execute_reply":"2025-03-10T13:17:24.609883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"test\" split to promptâ€“response pairs\nconverted_test = ds[\"test\"].map(format_prompt_response)\nconverted_test = converted_test.remove_columns(['subject_name', 'data', 'id'])\n# Convert the \"validation\" split\nconverted_validation = ds[\"validation\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['subject_name', 'data', 'id'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_test,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:17:26.866828Z","iopub.execute_input":"2025-03-10T13:17:26.867122Z","iopub.status.idle":"2025-03-10T13:17:26.930965Z","shell.execute_reply.started":"2025-03-10T13:17:26.867102Z","shell.execute_reply":"2025-03-10T13:17:26.929747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:17:41.488499Z","iopub.execute_input":"2025-03-10T13:17:41.488804Z","iopub.status.idle":"2025-03-10T13:17:41.542385Z","shell.execute_reply.started":"2025-03-10T13:17:41.488784Z","shell.execute_reply":"2025-03-10T13:17:41.540748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:17:55.998507Z","iopub.execute_input":"2025-03-10T13:17:55.998871Z","iopub.status.idle":"2025-03-10T13:17:58.368169Z","shell.execute_reply.started":"2025-03-10T13:17:55.998841Z","shell.execute_reply":"2025-03-10T13:17:58.366993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with mmlu medical genetics data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with mmlu medical genetics data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:18:48.314850Z","iopub.execute_input":"2025-03-10T13:18:48.315176Z","iopub.status.idle":"2025-03-10T13:18:54.066069Z","shell.execute_reply.started":"2025-03-10T13:18:48.315152Z","shell.execute_reply":"2025-03-10T13:18:54.064804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:22:34.488552Z","iopub.execute_input":"2025-03-10T13:22:34.488905Z","iopub.status.idle":"2025-03-10T13:22:38.965333Z","shell.execute_reply.started":"2025-03-10T13:22:34.488876Z","shell.execute_reply":"2025-03-10T13:22:38.963912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:22:41.218461Z","iopub.execute_input":"2025-03-10T13:22:41.218844Z","iopub.status.idle":"2025-03-10T13:22:41.225318Z","shell.execute_reply.started":"2025-03-10T13:22:41.218816Z","shell.execute_reply":"2025-03-10T13:22:41.223705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for MedQA","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"openlifescienceai/medqa\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:25:47.184686Z","iopub.execute_input":"2025-03-10T13:25:47.185022Z","iopub.status.idle":"2025-03-10T13:25:53.742462Z","shell.execute_reply.started":"2025-03-10T13:25:47.184995Z","shell.execute_reply":"2025-03-10T13:25:53.741571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"train\" split to promptâ€“response pairs\nconverted_test = ds[\"train\"].map(format_prompt_response)\nconverted_test = converted_test.remove_columns(['subject_name', 'data', 'id'])\n# Convert the \"test\" split\nconverted_validation = ds[\"test\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['subject_name', 'data', 'id'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_test,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:27:29.290459Z","iopub.execute_input":"2025-03-10T13:27:29.290791Z","iopub.status.idle":"2025-03-10T13:27:30.499547Z","shell.execute_reply.started":"2025-03-10T13:27:29.290764Z","shell.execute_reply":"2025-03-10T13:27:30.498093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:27:40.230258Z","iopub.execute_input":"2025-03-10T13:27:40.230615Z","iopub.status.idle":"2025-03-10T13:27:40.742469Z","shell.execute_reply.started":"2025-03-10T13:27:40.230587Z","shell.execute_reply":"2025-03-10T13:27:40.741360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_data[\"train\"] = new_data[\"train\"].map(lambda x: {'Category': 'medqa'})\nnew_data[\"test\"] = new_data[\"test\"].map(lambda x: {'Category': 'medqa'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:31:46.808650Z","iopub.execute_input":"2025-03-10T13:31:46.808969Z","iopub.status.idle":"2025-03-10T13:31:47.439488Z","shell.execute_reply.started":"2025-03-10T13:31:46.808948Z","shell.execute_reply":"2025-03-10T13:31:47.437953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:35:52.397219Z","iopub.execute_input":"2025-03-10T13:35:52.397649Z","iopub.status.idle":"2025-03-10T13:35:54.898264Z","shell.execute_reply.started":"2025-03-10T13:35:52.397606Z","shell.execute_reply":"2025-03-10T13:35:54.897083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with medqa data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with medqa data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:37:20.927652Z","iopub.execute_input":"2025-03-10T13:37:20.927949Z","iopub.status.idle":"2025-03-10T13:37:26.764013Z","shell.execute_reply.started":"2025-03-10T13:37:20.927929Z","shell.execute_reply":"2025-03-10T13:37:26.763069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:37:30.905938Z","iopub.execute_input":"2025-03-10T13:37:30.906260Z","iopub.status.idle":"2025-03-10T13:37:35.900543Z","shell.execute_reply.started":"2025-03-10T13:37:30.906233Z","shell.execute_reply":"2025-03-10T13:37:35.899384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:37:35.901887Z","iopub.execute_input":"2025-03-10T13:37:35.902157Z","iopub.status.idle":"2025-03-10T13:37:35.907145Z","shell.execute_reply.started":"2025-03-10T13:37:35.902133Z","shell.execute_reply":"2025-03-10T13:37:35.906052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for PubMedQA","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"openlifescienceai/pubmedqa\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:46:42.607660Z","iopub.execute_input":"2025-03-10T13:46:42.607945Z","iopub.status.idle":"2025-03-10T13:46:45.571415Z","shell.execute_reply.started":"2025-03-10T13:46:42.607927Z","shell.execute_reply":"2025-03-10T13:46:45.570260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds[\"train\"] = concatenate_datasets([ds[\"train\"], ds[\"test\"]])\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:46:47.157776Z","iopub.execute_input":"2025-03-10T13:46:47.158095Z","iopub.status.idle":"2025-03-10T13:46:47.171053Z","shell.execute_reply.started":"2025-03-10T13:46:47.158067Z","shell.execute_reply":"2025-03-10T13:46:47.169751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_template = \"\"\"You are a medical expert specialized in bio-medical research. Analyze the given context and answer the following question by selecting the correct or best option.\n\n### Context:\n{context}\n\n### Question:\n{question}\n\n### Options:\n(A) {option_A}\n(B) {option_B}\n(C) {option_C}\n\"\"\"\nresponse_template = \"\"\"Answer: ({correct_option}) {correct_answer}\nExplanation: {long_answer}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:58:40.314742Z","iopub.execute_input":"2025-03-10T13:58:40.315047Z","iopub.status.idle":"2025-03-10T13:58:40.319769Z","shell.execute_reply.started":"2025-03-10T13:58:40.315022Z","shell.execute_reply":"2025-03-10T13:58:40.318365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_prompt_response(record):\n    # Extract data from the 'data' dictionary.\n    data = record[\"data\"]\n    context = data.get(\"Context\", \"\")\n    question = data.get(\"Question\", \"\")\n    options = data.get(\"Options\", {})\n    option_A = options.get(\"A\", \"\")\n    option_B = options.get(\"B\", \"\")\n    option_C = options.get(\"C\", \"\")\n    prompt = prompt_template.format(\n        context=\"\\n\".join(context),\n        question=question,\n        option_A=option_A,\n        option_B=option_B,\n        option_C=option_C\n    )\n    correct_answer = data.get(\"Correct Answer\", \"\")\n    correct_option = data.get(\"Correct Option\", \"\")\n    long_answer = data.get(\"Long Answer\", \"\")\n    response = response_template.format(\n        correct_option=correct_option,\n        correct_answer=correct_answer,\n        long_answer=long_answer\n    )\n    return {\n        \"Category\": \"pubmedqa\",\n        \"ID\": record[\"id\"],\n        \"Prompt\": prompt,\n        \"Response\": response\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:59:09.893674Z","iopub.execute_input":"2025-03-10T13:59:09.893997Z","iopub.status.idle":"2025-03-10T13:59:09.899976Z","shell.execute_reply.started":"2025-03-10T13:59:09.893969Z","shell.execute_reply":"2025-03-10T13:59:09.898958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"train\" split to promptâ€“response pairs\nconverted_test = ds[\"train\"].map(format_prompt_response)\nconverted_test = converted_test.remove_columns(['data', 'id'])\n# Convert the \"validation\" split\nconverted_validation = ds[\"validation\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['data', 'id'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_test,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:01:02.190953Z","iopub.execute_input":"2025-03-10T14:01:02.191232Z","iopub.status.idle":"2025-03-10T14:01:02.358424Z","shell.execute_reply.started":"2025-03-10T14:01:02.191209Z","shell.execute_reply":"2025-03-10T14:01:02.357095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:01:30.009728Z","iopub.execute_input":"2025-03-10T14:01:30.010046Z","iopub.status.idle":"2025-03-10T14:01:30.097640Z","shell.execute_reply.started":"2025-03-10T14:01:30.010026Z","shell.execute_reply":"2025-03-10T14:01:30.096467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:02:42.911046Z","iopub.execute_input":"2025-03-10T14:02:42.911429Z","iopub.status.idle":"2025-03-10T14:02:45.533272Z","shell.execute_reply.started":"2025-03-10T14:02:42.911409Z","shell.execute_reply":"2025-03-10T14:02:45.532111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with pubmedqa data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with pubmedqa data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:03:30.413044Z","iopub.execute_input":"2025-03-10T14:03:30.413353Z","iopub.status.idle":"2025-03-10T14:03:36.409006Z","shell.execute_reply.started":"2025-03-10T14:03:30.413296Z","shell.execute_reply":"2025-03-10T14:03:36.407480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:03:45.674101Z","iopub.execute_input":"2025-03-10T14:03:45.674437Z","iopub.status.idle":"2025-03-10T14:03:50.506562Z","shell.execute_reply.started":"2025-03-10T14:03:45.674409Z","shell.execute_reply":"2025-03-10T14:03:50.505893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:03:50.513475Z","iopub.execute_input":"2025-03-10T14:03:50.513767Z","iopub.status.idle":"2025-03-10T14:03:50.519339Z","shell.execute_reply.started":"2025-03-10T14:03:50.513744Z","shell.execute_reply":"2025-03-10T14:03:50.518537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create dataset for MedMCQA","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"openlifescienceai/medmcqa\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:05:48.355698Z","iopub.execute_input":"2025-03-10T15:05:48.355980Z","iopub.status.idle":"2025-03-10T15:05:51.617081Z","shell.execute_reply.started":"2025-03-10T15:05:48.355960Z","shell.execute_reply":"2025-03-10T15:05:51.615398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keywords = [\n    'sleep', 'fitness', 'stress', 'heart', 'health', 'health care', 'personal', 'medical', 'cardio', 'medicine', 'exercise', 'smoking', 'smoker', 'alcohol', 'alcoholic', 'bmi', 'blood pressure', 'steps', 'step', 'run', 'jog', ' rem ', 'circadian', 'homeostatic', 'injury', ' train', 'etiology', 'etiological', 'recommend', 'advice', 'advise', 'assistance', 'assist', 'workout', 'work out', 'lifestyle', 'z-score', 'z score', 'athelete', 'athlete', 'sport', 'respiratory', 'fatigue', 'pressure', 'recover', 'hydrate', 'faint', 'drowsy', 'drowsiness', 'gym', 'muscle', 'sore', 'wake', 'rest', 'relax', 'insomnia', 'physic', 'care', 'calorie', 'fat loss', 'weight', 'height', 'mobility', 'activity', 'active'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:03:51.530572Z","iopub.execute_input":"2025-03-11T06:03:51.530895Z","iopub.status.idle":"2025-03-11T06:03:51.536335Z","shell.execute_reply.started":"2025-03-11T06:03:51.530870Z","shell.execute_reply":"2025-03-11T06:03:51.535311Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"len(keywords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:03:51.693456Z","iopub.execute_input":"2025-03-11T06:03:51.693823Z","iopub.status.idle":"2025-03-11T06:03:51.700229Z","shell.execute_reply.started":"2025-03-11T06:03:51.693789Z","shell.execute_reply":"2025-03-11T06:03:51.699093Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"65"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Function to check if any keyword is present in a record\ndef contains_keyword(example):\n    for keyword in keywords:\n        if keyword in example['question'].lower():\n            return True\n    return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply the filter to each split\nfor split in ds.keys():\n    ds[split] = ds[split].filter(contains_keyword)\n\nprint(ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:06:02.193994Z","iopub.execute_input":"2025-03-10T15:06:02.194377Z","iopub.status.idle":"2025-03-10T15:06:06.989459Z","shell.execute_reply.started":"2025-03-10T15:06:02.194343Z","shell.execute_reply":"2025-03-10T15:06:06.988489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_template = \"\"\"You are a medical expert specialized in {subject_name}. Answer the following question by selecting the correct or best option.\n\n### Question:\n{question}\n\n### Options:\n(A) {opa}\n(B) {opb}\n(C) {opc}\n(D) {opd}\n\"\"\"\nresponse_template = \"\"\"Answer: ({correct_option}) {correct_answer}\n{exp}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:06:17.861167Z","iopub.execute_input":"2025-03-10T15:06:17.861945Z","iopub.status.idle":"2025-03-10T15:06:17.866544Z","shell.execute_reply.started":"2025-03-10T15:06:17.861902Z","shell.execute_reply":"2025-03-10T15:06:17.865240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_prompt_response(record):\n    question = record[\"question\"]\n    opa = record[\"opa\"]\n    opb = record[\"opb\"]\n    opc = record[\"opc\"]\n    opd = record[\"opd\"]\n    subject_name = record['subject_name']\n    prompt = prompt_template.format(\n        subject_name=subject_name,\n        question=question,\n        opa=opa,\n        opb=opb,\n        opc=opc,\n        opd=opd\n    )\n    optionMap = {0:['A', opa], 1:['B', opb], 2:['C', opc], 3:['D', opd]}\n    correct_option = optionMap[record[\"cop\"]][0]\n    correct_answer = optionMap[record[\"cop\"]][1]\n    exp = \"\"\n    if record[\"exp\"]:\n        exp = f\"Explanation: {record['exp']}\" \n    response = response_template.format(\n        correct_option=correct_option,\n        correct_answer=correct_answer,\n        exp=exp\n    )\n    return {\n        \"Category\": \"MedMCQA\",\n        \"ID\": record[\"id\"],\n        \"Prompt\": prompt,\n        \"Response\": response\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:06:20.916903Z","iopub.execute_input":"2025-03-10T15:06:20.917178Z","iopub.status.idle":"2025-03-10T15:06:20.922535Z","shell.execute_reply.started":"2025-03-10T15:06:20.917159Z","shell.execute_reply":"2025-03-10T15:06:20.921663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the \"train\" split to promptâ€“response pairs\nconverted_train = ds[\"train\"].map(format_prompt_response)\nconverted_train = converted_train.remove_columns(['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'])\n# Convert the \"validation\" split\nconverted_validation = ds[\"validation\"].map(format_prompt_response)\nconverted_validation = converted_validation.remove_columns(['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_train,  # will be added to the \"train\" split\n    \"test\": converted_validation  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:06:38.647586Z","iopub.execute_input":"2025-03-10T15:06:38.647917Z","iopub.status.idle":"2025-03-10T15:06:42.249726Z","shell.execute_reply.started":"2025-03-10T15:06:38.647890Z","shell.execute_reply":"2025-03-10T15:06:42.249005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequential_ids(record, idx):\n    record['ID'] = idx+1\n    return record\n\n# Apply the function to both splits\nnew_data[\"train\"] = new_data[\"train\"].map(add_sequential_ids, with_indices=True)\nnew_data[\"test\"] = new_data[\"test\"].map(add_sequential_ids, with_indices=True)\n\nprint(new_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:06:45.468796Z","iopub.execute_input":"2025-03-10T15:06:45.469102Z","iopub.status.idle":"2025-03-10T15:06:46.387494Z","shell.execute_reply.started":"2025-03-10T15:06:45.469078Z","shell.execute_reply":"2025-03-10T15:06:46.386145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:09:09.095775Z","iopub.execute_input":"2025-03-10T15:09:09.096089Z","iopub.status.idle":"2025-03-10T15:09:11.504141Z","shell.execute_reply.started":"2025-03-10T15:09:09.096069Z","shell.execute_reply":"2025-03-10T15:09:11.503285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with medmcqa data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with medmcqa data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:09:43.830547Z","iopub.execute_input":"2025-03-10T15:09:43.830872Z","iopub.status.idle":"2025-03-10T15:09:51.824830Z","shell.execute_reply.started":"2025-03-10T15:09:43.830844Z","shell.execute_reply":"2025-03-10T15:09:51.823070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:10:35.550076Z","iopub.execute_input":"2025-03-10T15:10:35.550446Z","iopub.status.idle":"2025-03-10T15:10:41.971653Z","shell.execute_reply.started":"2025-03-10T15:10:35.550416Z","shell.execute_reply":"2025-03-10T15:10:41.970535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:10:44.298892Z","iopub.execute_input":"2025-03-10T15:10:44.299252Z","iopub.status.idle":"2025-03-10T15:10:44.305046Z","shell.execute_reply.started":"2025-03-10T15:10:44.299220Z","shell.execute_reply":"2025-03-10T15:10:44.303910Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# create dataset for Patient-Doctor interaction","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"ruslanmv/ai-medical-chatbot\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:04:29.574658Z","iopub.execute_input":"2025-03-11T06:04:29.575359Z","iopub.status.idle":"2025-03-11T06:04:33.982913Z","shell.execute_reply.started":"2025-03-11T06:04:29.575322Z","shell.execute_reply":"2025-03-11T06:04:33.982050Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec868b4757e54b09b5a10ca1cbe5aab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dialogues.parquet:   0%|          | 0.00/142M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17367e06b0c2448785cc7565292ba2ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/256916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b570b75599384eebb7586a0354b65bef"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:04:33.984142Z","iopub.execute_input":"2025-03-11T06:04:33.984479Z","iopub.status.idle":"2025-03-11T06:04:33.989954Z","shell.execute_reply.started":"2025-03-11T06:04:33.984452Z","shell.execute_reply":"2025-03-11T06:04:33.989049Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Description', 'Patient', 'Doctor'],\n        num_rows: 256916\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def contains_keyword(example):\n    for keyword in keywords:\n        if keyword in example['Description'].lower():\n            return True\n    return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:31:27.724662Z","iopub.execute_input":"2025-03-11T06:31:27.725087Z","iopub.status.idle":"2025-03-11T06:31:27.729594Z","shell.execute_reply.started":"2025-03-11T06:31:27.725053Z","shell.execute_reply":"2025-03-11T06:31:27.728614Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Apply the filter to each split\nfor split in ds.keys():\n    ds[split] = ds[split].filter(contains_keyword)\n\nprint(ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:31:50.058666Z","iopub.execute_input":"2025-03-11T06:31:50.059066Z","iopub.status.idle":"2025-03-11T06:31:55.769142Z","shell.execute_reply.started":"2025-03-11T06:31:50.059035Z","shell.execute_reply":"2025-03-11T06:31:55.768015Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/256916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6a1be95ed54ddca5093d509659d706"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['Description', 'Patient', 'Doctor'],\n        num_rows: 38164\n    })\n})\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"ds = ds[\"train\"].train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:34:35.000487Z","iopub.execute_input":"2025-03-11T06:34:35.000813Z","iopub.status.idle":"2025-03-11T06:34:35.027001Z","shell.execute_reply.started":"2025-03-11T06:34:35.000779Z","shell.execute_reply":"2025-03-11T06:34:35.026104Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:34:37.927048Z","iopub.execute_input":"2025-03-11T06:34:37.927367Z","iopub.status.idle":"2025-03-11T06:34:37.932924Z","shell.execute_reply.started":"2025-03-11T06:34:37.927344Z","shell.execute_reply":"2025-03-11T06:34:37.932031Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Description', 'Patient', 'Doctor'],\n        num_rows: 34347\n    })\n    test: Dataset({\n        features: ['Description', 'Patient', 'Doctor'],\n        num_rows: 3817\n    })\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"prompt_template = \"\"\"You are a compassionate and expert medical advisor specializing in personalized health assistance.\nAnalyze the context and the patientâ€™s query carefully, considering their background and concerns.\n\n### Patient Context:\n{description}\n\n### Patient Query:\n{patient_query}\n\nProvide a detailed, evidence-based, and empathetic response that offers practical recommendations.\n\"\"\"\n\nresponse_template = \"\"\"{doctor_response}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:35:16.345091Z","iopub.execute_input":"2025-03-11T06:35:16.345430Z","iopub.status.idle":"2025-03-11T06:35:16.349736Z","shell.execute_reply.started":"2025-03-11T06:35:16.345402Z","shell.execute_reply":"2025-03-11T06:35:16.348722Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def format_prompt_response(record, record_index):\n    description = record[\"Description\"]\n    patient_query = record[\"Patient\"]\n    doctor_response = record[\"Doctor\"]\n    \n    prompt = prompt_template.format(\n        description=description,\n        patient_query=patient_query\n    )\n    \n    response = response_template.format(\n        doctor_response=doctor_response\n    )\n    \n    return {\n        \"Category\": \"patient_doctor_conversation\",\n        \"ID\": record_index + 1,  # use record's index as ID\n        \"Prompt\": prompt,\n        \"Response\": response\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:41:51.930741Z","iopub.execute_input":"2025-03-11T06:41:51.931116Z","iopub.status.idle":"2025-03-11T06:41:51.936300Z","shell.execute_reply.started":"2025-03-11T06:41:51.931089Z","shell.execute_reply":"2025-03-11T06:41:51.935094Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Convert the \"train\" split to promptâ€“response pairs\nconverted_train = ds[\"train\"].map(format_prompt_response, with_indices=True)\nconverted_train = converted_train.remove_columns(['Description', 'Patient', 'Doctor'])\n# Convert the \"test\" split\nconverted_test = ds[\"test\"].map(format_prompt_response, with_indices=True)\nconverted_test = converted_test.remove_columns(['Description', 'Patient', 'Doctor'])\n# Create a new DatasetDict\nnew_data = DatasetDict({\n    \"train\": converted_train,  # will be added to the \"train\" split\n    \"test\": converted_test  # will be added to the \"test\" split\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:43:02.606299Z","iopub.execute_input":"2025-03-11T06:43:02.606620Z","iopub.status.idle":"2025-03-11T06:43:07.976361Z","shell.execute_reply.started":"2025-03-11T06:43:02.606596Z","shell.execute_reply":"2025-03-11T06:43:07.975231Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/34347 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d7356ebfc73489bbe309d50aba3fa3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3817 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d53cffd01af42d8b9349fcb168c8d78"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# print(new_data[\"train\"][-100])\n# print(new_data[\"test\"][-100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:48:58.521446Z","iopub.execute_input":"2025-03-11T06:48:58.521766Z","iopub.status.idle":"2025-03-11T06:48:58.525252Z","shell.execute_reply.started":"2025-03-11T06:48:58.521742Z","shell.execute_reply":"2025-03-11T06:48:58.524232Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Load the existing dataset from Hugging Face\nexisting_ds = load_dataset(\"johnjehiel/ph-llm-dataset\")\n\n# Append (concatenate) the new promptâ€“response pairs to the existing splits.\nupdated_train = concatenate_datasets([existing_ds[\"train\"], new_data[\"train\"]])\nupdated_test = concatenate_datasets([existing_ds[\"test\"], new_data[\"test\"]])\n\n# Create an updated DatasetDict\nupdated_ds = DatasetDict({\n    \"train\": updated_train,\n    \"test\": updated_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:48:17.679602Z","iopub.execute_input":"2025-03-11T06:48:17.679961Z","iopub.status.idle":"2025-03-11T06:48:20.959596Z","shell.execute_reply.started":"2025-03-11T06:48:17.679934Z","shell.execute_reply":"2025-03-11T06:48:20.958679Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"ph-llm-dataset_train.parquet:   0%|          | 0.00/19.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c2602c53484c91bd1106eb8241ef62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ph-llm-dataset_test.parquet:   0%|          | 0.00/1.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ae15c6c12a4be3942c8b2f1a67161c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32954 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01df7e33ca484a6cb39e1ec17895d278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2117 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b074cc76cd48d1ba2274bc8408ced6"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# Convert each split to a Parquet file (local temporary files)\ntrain_parquet_path = \"updated_train.parquet\"\ntest_parquet_path = \"updated_test.parquet\"\n\n# Save the splits to Parquet\nupdated_ds[\"train\"].to_parquet(train_parquet_path)\nupdated_ds[\"test\"].to_parquet(test_parquet_path)\n\nrepo_id = \"johnjehiel/ph-llm-dataset\"\ncreate_repo(repo_id=repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n\n# Upload the updated train split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=train_parquet_path,\n    path_in_repo=\"ph-llm-dataset_train.parquet\",  # using the existing train file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\", \n    commit_message=\"Update train split with patient-doctor conversation data\"\n)\n\n# Upload the updated test split using the existing file name in the repo\nupload_file(\n    path_or_fileobj=test_parquet_path,\n    path_in_repo=\"ph-llm-dataset_test.parquet\",  # using the existing test file name\n    repo_id=repo_id,\n    token=HF_TOKEN,\n    repo_type=\"dataset\",\n    commit_message=\"Update test split with patient-doctor conversation data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:53:38.690162Z","iopub.execute_input":"2025-03-11T06:53:38.690497Z","iopub.status.idle":"2025-03-11T06:53:46.382433Z","shell.execute_reply.started":"2025-03-11T06:53:38.690472Z","shell.execute_reply":"2025-03-11T06:53:46.381476Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/68 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157ea01033524f8586440bbb45ec5477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb4e3447e0fc4a74ab38464aec1b6fdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"updated_train.parquet:   0%|          | 0.00/43.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43789060e03d4cb680f97d8682cc1267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"updated_test.parquet:   0%|          | 0.00/3.89M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfb6a15b69f64aa9b3703415f641e6f6"}},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/datasets/johnjehiel/ph-llm-dataset/commit/53cc68972d85a823e6b6baa8b3c8e6dc61d065ae', commit_message='Update test split with patient-doctor conversation data', commit_description='', oid='53cc68972d85a823e6b6baa8b3c8e6dc61d065ae', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/johnjehiel/ph-llm-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='johnjehiel/ph-llm-dataset'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":33}]}